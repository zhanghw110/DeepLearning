## 1. 什么是激活函数
在神经网络中，**对于某一个隐藏层的节点**的激活值计算一般分为**两步**：

（1）**线性变换**;根据**输入**该节点的值 x1,x2，计算出**加权输入**值 z<sup>[1]</sup>=w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub>+b<sup>[1]</sup>=W<sup>[1]</sup>x+b<sup>[1]</sup>z<sup>[1]</sup> ，上标 1表示第1层隐藏层;w表示权重，b表示偏移量。

![sigmoid函数](/img/1.1activefun.jpeg)

（2）将**加权输入值**进行**非线性变换**，也就是经过**激活函数**，计算出该节点的**输出值(激活值)** a<sup>(1)</sup>=g(z<sup>(1)</sup>)，其中 g(z) 为非线性函数。
## 2. 常用的激活函数
在深度学习中，常用的激活函数主要有：

+ sigmoid函数，
+ tanh函数，
- ReLU函数。

2.1 sigmoid函数

该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间,可以用来做二分类。在特征相差比较复杂或是相差不是特别大的时效果较好。

sigmoid函数的公式：


图形如下：


![sigmoid函数手绘输出](/img/sigmoid.jpg)
