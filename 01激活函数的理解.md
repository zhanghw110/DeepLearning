## 1. 什么是激活函数
在神经网络中，我们经常可以看到**对于某一个隐藏层的节点**，该节点的激活值计算一般分为**两步**：

（1）输入该节点的值为 x1,x2x1,x2 时，在进入这个隐藏节点后，会先进行一个**线性变换**，计算出值 z<sup>[1]</sup>=w<sub>1</sub>x<sub>1</sub>+w<sub>2</sub>x<sub>2</sub>+b<sup>[1]</sup>=W<sup>[1]</sup>x+b<sup>[1]</sup>z<sup>[1]</sup> ，上标 1表示第1层隐藏层。

![sigmoid函数](/img/sigmoid函数.png)

（2）再进行一个**非线性变换**，也就是经过**非线性激活函数**，计算出该节点的**输出值(激活值)** a<sup>(1)</sup>=g(z<sup>(1)</sup>)，其中 g(z) 为非线性函数。
## 2. 常用的激活函数
在深度学习中，常用的激活函数主要有：sigmoid函数，tanh函数，ReLU函数。下面我们将一一介绍。

2.1 sigmoid函数
在逻辑回归中我们介绍过sigmoid函数，该函数是将取值为 (−∞,+∞)(−∞,+∞) 的数映射到 (0,1)(0,1) 之间。sigmoid函数的公式以及图形如下：

$\dfract{\trfract{1}{2}}

g(z)=`$\dfract{\tfract{1}1+e−z

